import { ChatOllama } from '@langchain/ollama';
import dotenv from 'dotenv';

dotenv.config();

async function testOllama() {
  console.log('üß™ Testing Ollama connection...\n');
  
  const llm = new ChatOllama({
    baseUrl: process.env.OLLAMA_BASE_URL || 'http://localhost:11434',
    model: process.env.OLLAMA_MODEL || 'llama3.1:8b',
    temperature: 0.1,
  });

  try {
    console.log(`üì° Connecting to: ${process.env.OLLAMA_BASE_URL || 'http://localhost:11434'}`);
    console.log(`ü§ñ Using model: ${process.env.OLLAMA_MODEL || 'llama3.1:8b'}\n`);
    
    const response = await llm.invoke(
      "Explain what Adobe Experience Platform is in one sentence."
    );
    
    console.log('‚úÖ Ollama is working!\n');
    console.log('Response:', response);
    console.log('\n‚ú® Success! LangChain can communicate with Ollama.');
  } catch (error) {
    console.error('‚ùå Error:', error.message);
    console.log('\nüí° Troubleshooting:');
    console.log('   1. Make sure Ollama is running: ollama serve');
    console.log('   2. Check if model is downloaded: ollama list');
    console.log('   3. Verify OLLAMA_BASE_URL in .env');
  }
}

testOllama();